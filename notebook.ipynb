{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae4203c-dce1-42b1-a639-0999f04fb413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29ebe2c-f715-45aa-a28f-8fcdd5d0c088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52ad96c-2c30-48ae-a8e7-eb815c9c969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 5110 non-null   int64  \n",
      " 1   gender             5110 non-null   object \n",
      " 2   age                5110 non-null   float64\n",
      " 3   hypertension       5110 non-null   int64  \n",
      " 4   heart_disease      5110 non-null   int64  \n",
      " 5   ever_married       5110 non-null   object \n",
      " 6   work_type          5110 non-null   object \n",
      " 7   Residence_type     5110 non-null   object \n",
      " 8   avg_glucose_level  5110 non-null   float64\n",
      " 9   bmi                4909 non-null   float64\n",
      " 10  smoking_status     5110 non-null   object \n",
      " 11  stroke             5110 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 479.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5bc482-65b2-441f-bc04-16ecd796101a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stroke\n",
       "0    4861\n",
       "1     249\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0596a-7614-42c6-8e57-b17a1243ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the output from the cell above tells us a few things:\n",
    "1. the dataset is highly imbalanced\n",
    "2. testing the result with an accuracy score would be useless. why?\n",
    "3. if we just predict no all the time, our model would be 4861/5110 = 95.1% accurate!\n",
    "4. so to test our models performance, we will be using metrics like:\n",
    "    - precision and recall\n",
    "    - f1-score\n",
    "    - roc-auc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4654621-5e0b-47d4-b362-554c7079d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_mean = df['bmi'].mean()\n",
    "df['bmi'] = df['bmi'].fillna(bmi_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de9a5bf-0190-45b7-a5ca-ca33bbbb6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 5110 non-null   int64  \n",
      " 1   gender             5110 non-null   object \n",
      " 2   age                5110 non-null   float64\n",
      " 3   hypertension       5110 non-null   int64  \n",
      " 4   heart_disease      5110 non-null   int64  \n",
      " 5   ever_married       5110 non-null   object \n",
      " 6   work_type          5110 non-null   object \n",
      " 7   Residence_type     5110 non-null   object \n",
      " 8   avg_glucose_level  5110 non-null   float64\n",
      " 9   bmi                5110 non-null   float64\n",
      " 10  smoking_status     5110 non-null   object \n",
      " 11  stroke             5110 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 479.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c81480f8-4d16-4361-a8a7-b425142a0b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smoking_status\n",
       "never smoked       1892\n",
       "Unknown            1544\n",
       "formerly smoked     885\n",
       "smokes              789\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['smoking_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c78ae-071e-4cfe-bb33-5f18a7644e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "there are too many rows with 'Unknown' as a smoking status.\n",
    "so instead of dropping or manipulating the data, we will treat it as it's own category\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4caa3fc-aaae-4766-9cb6-63365815b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "255eab6a-e7c4-4ece-b88c-1d8228e801a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aae5b1c-d753-45c1-aeef-cca86dec7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6db5c30f-ccf5-4072-b450-851db9541e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3066, 1022, 1022)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
    "\n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82200a0b-2714-4399-8bad-88fcb5b2be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable (y)\n",
    "y_train = df_train.stroke.values\n",
    "y_val = df_val.stroke.values\n",
    "y_test = df_test.stroke.values\n",
    "\n",
    "# Remove target from the feature dataframes\n",
    "del df_train['stroke']\n",
    "del df_val['stroke']\n",
    "del df_test['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ef89a0e-f5f9-4ab9-8fb7-6f583943bb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Mutual Information Scores ---\n",
      "'gender': 0.0\n",
      "'ever_married': 0.01\n",
      "'work_type': 0.01\n",
      "'residence_type': 0.0\n",
      "'smoking_status': 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# We will calculate MI only for our categorical features\n",
    "# using only the training data\n",
    "# MI score helps us know which features have more say on the outcome\n",
    "categorical = ['gender', 'ever_married', 'work_type', 'residence_type', 'smoking_status']\n",
    "print(\"--- Mutual Information Scores ---\")\n",
    "for col in categorical:\n",
    "    score = mutual_info_score(df_train[col], y_train)\n",
    "    print(f\"'{col}': {round(score, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c55cc-d461-4af9-a0d5-df88a65b8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "it seems no feature holds more importance than the rest\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15fb2212-1165-41a4-8580-d304858602cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer(sparse=True)\n",
    "\n",
    "train_dicts = df_train.to_dict(orient='records')\n",
    "val_dicts = df_val.to_dict(orient='records')\n",
    "\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0898721-ed73-47e9-8382-7e6f5ca7da3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=42, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a25a796-68bc-44f5-bb2b-6fddf2a38891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8311721509574269\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 7. the predicted probabilities for the validation set\n",
    "# We need the probabilities for the \"positive\" class (1)\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df0394-9c1c-4b8f-a72d-bd5400291e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "our auc score of 0.83117 is our baseline score, from our logistic regression model\n",
    "it's a good score, but we know that we can do better other tree-based models\n",
    "next up, we'll be trying our random forest regression\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e307925-9744-40e1-a032-229be183fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning the 'C' parameter ---\n",
      "C = 0.001: \t AUC score = 0.6402445\n",
      "C = 0.01: \t AUC score = 0.682097\n",
      "C = 0.1: \t AUC score = 0.7907371\n",
      "C = 1: \t AUC score = 0.8311722\n",
      "C = 10: \t AUC score = 0.8348206\n",
      "C = 35: \t AUC score = 0.8359128\n",
      "C = 50: \t AUC score = 0.8358199\n",
      "C = 100: \t AUC score = 0.8357037\n",
      "C = 200: \t AUC score = 0.8357037\n"
     ]
    }
   ],
   "source": [
    "# we're back. and now finetuning logistic reg to see what value of c works best\n",
    "\n",
    "# The list of C values to test\n",
    "c_values = [0.001, 0.01, 0.1, 1, 10, 35, 50, 100, 200]\n",
    "# c_values = [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 47, 49, 50]\n",
    "\n",
    "print(\"--- Tuning the 'C' parameter ---\")\n",
    "\n",
    "# Loop through each C value\n",
    "for c in c_values:\n",
    "    # 1. Initialize the model with the specific C value\n",
    "    model = LogisticRegression(solver='liblinear', C=c, max_iter=1000, random_state=42)\n",
    "    \n",
    "    # 2. Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. Calculate auc score on the validation set\n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    # 4. Print the result\n",
    "    print(f\"C = {c}: \\t AUC score = {round(auc, 7)}\") # \\t adds a nice tab for alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de950afd-e304-4352-a4a6-158399e6f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "C=35 gives us the highest auc score\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8caf5e0-7ec4-4bf0-bb32-ccbe1bbb20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier trained successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Classifier trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "639bd82a-7375-4896-85fd-aa18b04c3e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7673126975274214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = rf_model.predict_proba(X_val)[:, 1]\n",
    "rf_auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83d8cd-fc1a-47c3-9660-b5983f9c31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the default settings for random forest perform poorly, compared to logistic regression\n",
    "but we can fine tune it later\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1d052f1-b84e-4790-a6db-a641689b9cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier trained successfully!\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost Classifier trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad3338f3-aab9-403c-9c19-92ac800a9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7836958542480014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = xgb_model.predict_proba(X_val)[:, 1]\n",
    "rf_auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c14d31-e565-41dd-8db9-f150832863ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i have learnt that for an imbalanced dataset like ours, tuning the 'scale_pos_weight' parameter is important\n",
    "currently, it weighs the negative(no stroke) and positive (has_stroke) data equally. the goal of using this parameter\n",
    "is to ensure that the model weighs them equally\n",
    "the standard way to calculate it is: count(negative_class)/count(positive_class)\n",
    "in our case, that's 4861 / 249, which is ~19.5.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d98fcbd-e303-46ce-948a-3104b0944feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051682468860383\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, scale_pos_weight=19.5)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict_proba(X_val)[:, 1]\n",
    "rf_auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b772674-8011-4c6d-b1c5-18619617e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i have also learnt a better way to tune your xgboost model using GridSearchCV\n",
    "so instead of manually tuning with multiple loops, we let gridsearchcv handle\n",
    "all that work for us, and figure out the best set of parameters from our param_grid\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0bc04e7-472e-44eb-8e30-bb55f12064b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training data (80%) is now vectorized.\n",
      "--- Starting Grid Search (this may take a minute...) ---\n",
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n",
      "\n",
      "--- Grid Search Complete! ---\n",
      "Best AUC Score: 0.8437\n",
      "Best Parameters Found:\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Create 'y' by copying the 'stroke' column\n",
    "y_full_train = df_full_train['stroke'].values\n",
    "\n",
    "# Create 'X' by dropping 'stroke'. This makes a new copy.\n",
    "X_full_train_df = df_full_train.drop('stroke', axis=1)\n",
    "\n",
    "# --- 2. Vectorize our new X_full_train_df ---\n",
    "dv_full = DictVectorizer(sparse=True)\n",
    "X_full_train = dv_full.fit_transform(X_full_train_df.to_dict(orient='records'))\n",
    "\n",
    "print(\"Full training data (80%) is now vectorized.\")\n",
    "\n",
    "# --- 3. Define our parameter grid ---\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],         # Try simple, medium, and complex trees\n",
    "    'learning_rate': [0.1, 0.05],   # Try a slow and a very slow rate\n",
    "    'min_child_weight': [1, 5, 10]    # Try different anti-overfitting values\n",
    "}\n",
    "\n",
    "# --- 4. Create our base XGBoost model ---\n",
    "xgb_model_tuned = XGBClassifier(\n",
    "    scale_pos_weight=19.5, \n",
    "    random_state=42,\n",
    "    n_estimators=100  # We can add this to make sure it has enough trees\n",
    ")\n",
    "\n",
    "# --- 5. Set up the Grid Search ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model_tuned, \n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1  # This will print updates so you know it's working\n",
    ")\n",
    "\n",
    "# --- 6. Run the Grid Search! ---\n",
    "# This will take a few minutes. It's training (3*2*3) * 5 = 90 models!\n",
    "print(\"--- Starting Grid Search (this may take a minute...) ---\")\n",
    "grid_search.fit(X_full_train, y_full_train)\n",
    "\n",
    "# --- 7. Print the Best Results ---\n",
    "print(\"\\n--- Grid Search Complete! ---\")\n",
    "print(f\"Best AUC Score: {grid_search.best_score_:.4f}\")\n",
    "print(\"Best Parameters Found:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee87b34c-8411-45d3-b89a-d383c0aa0fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.843650453187147"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8da328-a1e1-4f52-b3ba-46c54f53bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we've tuned our xgboost model to have a better roc score than our logistic regression model.\n",
    "next, we explore another validation approach to confirm which model performs better\n",
    "Precision-Recall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8524dbb3-23a9-4b80-98d9-9221b7626c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metrics @ 0.5 Threshold ---\n",
      "\n",
      "Logistic Regression (C=35):\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.0227\n",
      "  F1-Score:  0.0444\n",
      "\n",
      "Tuned XGBoost:\n",
      "  Precision: 0.1285\n",
      "  Recall:    0.7273\n",
      "  F1-Score:  0.2184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- 1. Train our BEST Logistic Regression Model ---\n",
    "# (We use the C=35 we found from tuning)\n",
    "model_lr = LogisticRegression(solver='liblinear', C=35, max_iter=1000, random_state=42)\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_proba_lr = model_lr.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# --- 2. Train our BEST XGBoost Model ---\n",
    "# (We use the parameters we found from the Grid Search)\n",
    "model_xgb = XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=2,\n",
    "    min_child_weight=10,\n",
    "    scale_pos_weight=19.5,  # The key imbalance parameter\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_proba_xgb = model_xgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "\n",
    "# --- 3. Convert probabilities to 0/1 predictions using the 0.5 threshold ---\n",
    "y_pred_lr = (y_pred_proba_lr >= 0.5).astype(int)\n",
    "y_pred_xgb = (y_pred_proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "# --- 4. Calculate and Print Metrics ---\n",
    "print(\"--- Metrics @ 0.5 Threshold ---\")\n",
    "\n",
    "# Logistic Regression\n",
    "precision_lr = precision_score(y_val, y_pred_lr)\n",
    "recall_lr = recall_score(y_val, y_pred_lr)\n",
    "f1_lr = f1_score(y_val, y_pred_lr)\n",
    "\n",
    "print(f\"\\nLogistic Regression (C=35):\")\n",
    "print(f\"  Precision: {precision_lr:.4f}\")\n",
    "print(f\"  Recall:    {recall_lr:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_lr:.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "precision_xgb = precision_score(y_val, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_val, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_val, y_pred_xgb)\n",
    "\n",
    "print(f\"\\nTuned XGBoost:\")\n",
    "print(f\"  Precision: {precision_xgb:.4f}\")\n",
    "print(f\"  Recall:    {recall_xgb:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b47392-6c29-45c8-98c4-6cdba78e5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the logistic regression model (c=35) is effectively useless because it is too cautious. \n",
    "while its 1.0 precision seems perfect, this score is deceptive. it's achieved by a model that almost never predicts \"stroke.\"\n",
    "this is confirmed by its extremely low recall of 0.0227, meaning it fails to identify over 97% of the actual stroke patients,\n",
    "thereby failing at the project's main goal.\n",
    "\n",
    "the tuned xgboost model, in sharp contrast, is optimized for finding positive cases. it achieves a very high recall of 0.7273,\n",
    "successfully identifying nearly 73% of all actual stroke patients. this usefulness comes at the cost of low precision (0.1285),\n",
    "as it flags many non-stroke patients as well. despite the high number of false positives, this model is far more practical because\n",
    "it actually finds the at-risk individuals.\n",
    "\n",
    "in summary, we need to find a good threshold for both models\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6b2349c-9d36-4cc3-be4e-7a4805e086b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best F1-Score (after tuning threshold) ---\n",
      "\n",
      "Logistic Regression (C=35):\n",
      "  Best F1-Score: 0.2489\n",
      "  At Threshold:  0.09\n",
      "\n",
      "Tuned XGBoost:\n",
      "  Best F1-Score: 0.2411\n",
      "  At Threshold:  0.59\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# A list to store scores for our LR model\n",
    "scores_lr = []\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "# Loop through all thresholds for Logistic Regression\n",
    "for t in thresholds:\n",
    "    y_pred_lr = (y_pred_proba_lr >= t).astype(int)\n",
    "    f1 = f1_score(y_val, y_pred_lr)\n",
    "    scores_lr.append((t, f1))\n",
    "\n",
    "# Find the best LR score\n",
    "scores_lr.sort(key=lambda x: x[1], reverse=True)\n",
    "best_f1_lr = scores_lr[0][1]\n",
    "best_thresh_lr = scores_lr[0][0]\n",
    "\n",
    "# --- Now do the same for XGBoost ---\n",
    "scores_xgb = []\n",
    "for t in thresholds:\n",
    "    y_pred_xgb = (y_pred_proba_xgb >= t).astype(int)\n",
    "    f1 = f1_score(y_val, y_pred_xgb)\n",
    "    scores_xgb.append((t, f1))\n",
    "\n",
    "# Find the best XGB score\n",
    "scores_xgb.sort(key=lambda x: x[1], reverse=True)\n",
    "best_f1_xgb = scores_xgb[0][1]\n",
    "best_thresh_xgb = scores_xgb[0][0]\n",
    "\n",
    "\n",
    "# --- Print The Final Results ---\n",
    "print(\"--- Best F1-Score (after tuning threshold) ---\")\n",
    "\n",
    "print(f\"\\nLogistic Regression (C=35):\")\n",
    "print(f\"  Best F1-Score: {best_f1_lr:.4f}\")\n",
    "print(f\"  At Threshold:  {best_thresh_lr:.2f}\")\n",
    "\n",
    "print(f\"\\nTuned XGBoost:\")\n",
    "print(f\"  Best F1-Score: {best_f1_xgb:.4f}\")\n",
    "print(f\"  At Threshold:  {best_thresh_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed519e7-4909-4da1-90e8-63324515d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final Model Selection: Logistic Regression\n",
    "\n",
    "After tuning the decision threshold to find the best F1-Score for both models, the Logistic Regression (C=35) is the clear winner.\n",
    "\n",
    "1. Final Performance (Best F1-Score): The models' peak performances were virtually tied, with the Logistic Regression just slightly ahead.\n",
    "    Logistic Regression (C=35):\n",
    "        Best F1-Score: 0.2489\n",
    "        At Threshold: 0.09\n",
    "    Tuned XGBoost:\n",
    "        Best F1-Score: 0.2411\n",
    "        At Threshold: 0.59\n",
    "\n",
    "2. The Threshold Story: This result shows both models achieving the same balance from opposite directions. The \"cautious\" Logistic Regression\n",
    "    needed its threshold lowered to 0.09 to become more aggressive, while the \"over-eager\" XGBoost needed its threshold raised to 0.59 to become more\n",
    "    conservative.\n",
    "\n",
    "3. Final Decision: Why Logistic Regression Wins Despite the similar F1-Scores, Logistic Regression is the undisputed champion for four key reasons:\n",
    "    Performance: It has the (slightly) higher F1-Score.\n",
    "    Simplicity: It is a dramatically simpler model than XGBoost.\n",
    "    Cost: It is thousands of times faster and cheaper to train and re-train.\n",
    "    Interpretability: It is an interpretable \"glass box\" model. This is the most critical factor, as it allows us to explain why a risk score is high,\n",
    "    which is essential for a medical application.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7de5fe20-9f11-4304-8e80-237b831e0d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained successfully.\n",
      "Vectorizer saved to dv.bin\n",
      "Model saved to model.bin\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "final_model = LogisticRegression(solver='liblinear', C=35, max_iter=1000, random_state=42)\n",
    "final_model.fit(X_full_train, y_full_train)\n",
    "print(\"Final model trained successfully.\")\n",
    "\n",
    "# Save the vectorizer (dv_full)\n",
    "with open('dv.bin', 'wb') as f_out:\n",
    "    pickle.dump(dv_full, f_out)\n",
    "    print(\"Vectorizer saved to dv.bin\")\n",
    "\n",
    "# Save the final model\n",
    "with open('model.bin', 'wb') as f_out:\n",
    "    pickle.dump(final_model, f_out)\n",
    "    print(\"Model saved to model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86cb94-e086-45ff-99b0-4b05b7656888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
